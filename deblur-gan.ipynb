{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1118216,"sourceType":"datasetVersion","datasetId":627736}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import library**","metadata":{}},{"cell_type":"code","source":"pip install scikit-image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T13:58:56.644927Z","iopub.execute_input":"2025-06-03T13:58:56.645114Z","iopub.status.idle":"2025-06-03T13:59:00.462186Z","shell.execute_reply.started":"2025-06-03T13:58:56.645099Z","shell.execute_reply":"2025-06-03T13:59:00.461357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms, utils\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\nimport torchvision.models as models\nimport gc\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\nbase_path = \"/kaggle/input/gopro-deblur/gopro_deblur\"\nblur_dir = os.path.join(base_path, \"blur\", \"images\")\nsharp_dir = os.path.join(base_path, \"sharp\", \"images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T13:59:00.463166Z","iopub.execute_input":"2025-06-03T13:59:00.463399Z","iopub.status.idle":"2025-06-03T13:59:08.397350Z","shell.execute_reply.started":"2025-06-03T13:59:00.463375Z","shell.execute_reply":"2025-06-03T13:59:08.396783Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Dataset preview**","metadata":{}},{"cell_type":"code","source":"image_pairs = []\nimage_exts = (\".png\", \".jpg\", \".jpeg\", \".bmp\")\nfor filename in os.listdir(blur_dir):\n    if filename.lower().endswith(image_exts):\n        blur_path = os.path.join(blur_dir, filename)\n        sharp_path = os.path.join(sharp_dir, filename)\n        if os.path.isfile(blur_path) and os.path.isfile(sharp_path):\n            image_pairs.append((blur_path, sharp_path))\n\nprint(f\"Total image pairs: {len(image_pairs)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T13:59:08.398836Z","iopub.execute_input":"2025-06-03T13:59:08.399150Z","iopub.status.idle":"2025-06-03T13:59:15.981320Z","shell.execute_reply.started":"2025-06-03T13:59:08.399132Z","shell.execute_reply":"2025-06-03T13:59:15.980670Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"blur_path, sharp_path = image_pairs[0]\n\nblur_img = Image.open(blur_path).convert(\"RGB\")\nsharp_img = Image.open(sharp_path).convert(\"RGB\")\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.title(\"Blurry Image\")\nplt.imshow(blur_img)\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\nplt.title(\"Sharp Image\")\nplt.imshow(sharp_img)\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T13:59:15.982016Z","iopub.execute_input":"2025-06-03T13:59:15.982245Z","iopub.status.idle":"2025-06-03T13:59:16.769952Z","shell.execute_reply.started":"2025-06-03T13:59:15.982226Z","shell.execute_reply":"2025-06-03T13:59:16.769194Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data loader**","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\nclass DeblurDataset(Dataset):\n    def __init__(self, image_pairs, transform=None):\n        self.image_pairs = image_pairs\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_pairs)\n\n    def __getitem__(self, idx):\n        blur_path, sharp_path = self.image_pairs[idx]\n        blur_img = Image.open(blur_path).convert(\"RGB\")\n        sharp_img = Image.open(sharp_path).convert(\"RGB\")\n        if self.transform:\n            blur_img = self.transform(blur_img)\n            sharp_img = self.transform(sharp_img)\n        return blur_img, sharp_img\n\ndataset = DeblurDataset(image_pairs, transform=transform)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T13:59:16.770819Z","iopub.execute_input":"2025-06-03T13:59:16.771035Z","iopub.status.idle":"2025-06-03T13:59:16.779573Z","shell.execute_reply.started":"2025-06-03T13:59:16.771017Z","shell.execute_reply":"2025-06-03T13:59:16.778871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def postprocess_deblurred_image(pred, device):\n    import torch.nn.functional as F\n    import cv2\n    import torch\n\n    def unsharp_mask(img, amount=1.2):\n        blurred = F.avg_pool2d(img, kernel_size=3, stride=1, padding=1)\n        mask = img - blurred\n        return torch.clamp(img + amount * mask, -1, 1)\n\n    def gaussian_smooth(img, kernel_size=3, sigma=0.6):\n        B, C, H, W = img.shape\n        k = cv2.getGaussianKernel(kernel_size, sigma)\n        kernel = torch.tensor(k @ k.T, dtype=torch.float32, device=device)\n        kernel = kernel.expand(C, 1, kernel_size, kernel_size)\n        return F.conv2d(img, kernel, padding=kernel_size // 2, groups=C)\n\n    sharp = unsharp_mask(pred, amount=1.2)\n    smoothed = gaussian_smooth(sharp)\n\n    edge_strength = torch.abs(pred - F.avg_pool2d(pred, 3, 1, 1))\n    edge_mask = torch.sigmoid(edge_strength.mean(dim=1, keepdim=True) * 10)\n\n    blended = smoothed * (1 - edge_mask) + sharp * edge_mask\n    return blended.clamp(-1, 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T13:59:16.780473Z","iopub.execute_input":"2025-06-03T13:59:16.780927Z","iopub.status.idle":"2025-06-03T13:59:16.793097Z","shell.execute_reply.started":"2025-06-03T13:59:16.780903Z","shell.execute_reply":"2025-06-03T13:59:16.792450Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **DeblurGAN**","metadata":{}},{"cell_type":"markdown","source":"## Residual block","metadata":{}},{"cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(channels, channels, 3, padding=1),\n            nn.InstanceNorm2d(channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, channels, 3, padding=1),\n            nn.InstanceNorm2d(channels)\n        )\n\n    def forward(self, x):\n        return x + self.block(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T13:59:16.793759Z","iopub.execute_input":"2025-06-03T13:59:16.793982Z","iopub.status.idle":"2025-06-03T13:59:16.810890Z","shell.execute_reply.started":"2025-06-03T13:59:16.793963Z","shell.execute_reply":"2025-06-03T13:59:16.810234Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generator","metadata":{}},{"cell_type":"code","source":"class DeblurGenerator(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, num_res_blocks=6):\n        super().__init__()\n        model = [\n            nn.Conv2d(in_channels, 64, 7, padding=3),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n            nn.InstanceNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n            nn.InstanceNorm2d(256),\n            nn.ReLU(inplace=True),\n        ]\n        model += [ResBlock(256) for _ in range(num_res_blocks)]\n        model += [\n            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n            nn.InstanceNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, out_channels, 7, padding=3),\n            nn.Tanh()\n        ]\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return torch.clamp(x + self.model(x),min=-1, max=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T13:59:16.811502Z","iopub.execute_input":"2025-06-03T13:59:16.811733Z","iopub.status.idle":"2025-06-03T13:59:16.821145Z","shell.execute_reply.started":"2025-06-03T13:59:16.811709Z","shell.execute_reply":"2025-06-03T13:59:16.820474Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Discriminator","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, in_channels=3):\n        super().__init__()\n        def block(in_feat, out_feat, norm=True):\n            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if norm:\n                layers.append(nn.InstanceNorm2d(out_feat))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(in_channels, 64, norm=False),\n            *block(64, 128),\n            *block(128, 256),\n            *block(256, 512),\n            nn.Conv2d(512, 1, kernel_size=4, padding=1)\n        )\n\n    def forward(self, img):\n        return self.model(img)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T13:59:16.823321Z","iopub.execute_input":"2025-06-03T13:59:16.823767Z","iopub.status.idle":"2025-06-03T13:59:16.839147Z","shell.execute_reply.started":"2025-06-03T13:59:16.823741Z","shell.execute_reply":"2025-06-03T13:59:16.838366Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loss function","metadata":{}},{"cell_type":"code","source":"class VGGPerceptualLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features\n        self.feature_extractor = nn.Sequential(*list(vgg)[:16]).eval()\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = False\n        self.loss = nn.L1Loss()\n\n    def forward(self, pred, target):\n        pred_features = self.feature_extractor(pred)\n        target_features = self.feature_extractor(target)\n        return self.loss(pred_features, target_features)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngenerator = DeblurGenerator().to(device)\ndiscriminator = Discriminator().to(device)\n\nadv_criterion = nn.MSELoss()\ncontent_criterion = VGGPerceptualLoss().to(device)\ng_optimizer = optim.Adam(generator.parameters(), lr=1e-4)\nd_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4)\nos.makedirs(\"checkpoints\", exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T13:59:16.839964Z","iopub.execute_input":"2025-06-03T13:59:16.840202Z","iopub.status.idle":"2025-06-03T13:59:22.268916Z","shell.execute_reply.started":"2025-06-03T13:59:16.840182Z","shell.execute_reply":"2025-06-03T13:59:22.268112Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## evaluate model","metadata":{}},{"cell_type":"code","source":"# def evaluate_psnr_ssim(model, dataloader, device, epoch):\n#     model.eval()\n#     psnr_total, ssim_total = 0, 0\n#     with torch.no_grad():\n#         for blur, sharp in dataloader:\n#             blur, sharp = blur.to(device), sharp.to(device)\n\n#             pred = model(blur).clamp(-1, 1)\n\n#             # pred = postprocess_deblurred_image(pred, device)\n\n#             for i in range(pred.size(0)):\n#                 p = pred[i].cpu().numpy().transpose(1, 2, 0)\n#                 s = sharp[i].cpu().numpy().transpose(1, 2, 0)\n#                 p = (p + 1) / 2\n#                 s = (s + 1) / 2\n#                 psnr_total += peak_signal_noise_ratio(s, p, data_range=1)\n#                 ssim_total += structural_similarity(s, p, channel_axis=2, data_range=1)\n\n#         n = len(dataloader.dataset)\n#         print(f\"\\n[Evaluation @ Epoch {epoch}] PSNR: {psnr_total / n:.2f}, SSIM: {ssim_total / n:.4f}\\n\")\n\n#         blur_img = blur[0].cpu().permute(1, 2, 0).numpy()\n#         sharp_img = sharp[0].cpu().permute(1, 2, 0).numpy()\n#         pred_img = pred[0].cpu().permute(1, 2, 0).numpy()\n\n#         imgs = [(blur_img + 1) / 2, (pred_img + 1) / 2, (sharp_img + 1) / 2]\n#         titles = [\"Blurry\", \"Predicted\", \"Sharp\"]\n#         plt.figure(figsize=(12, 4))\n#         for i in range(3):\n#             plt.subplot(1, 3, i + 1)\n#             plt.imshow(imgs[i])\n#             plt.title(titles[i])\n#             plt.axis(\"off\")\n#         plt.tight_layout()\n#         plt.show()\n\n\n# batch_size = 4\n\n# for epoch in range(100):\n#     generator.train()\n#     discriminator.train()\n    \n#     for i, (blur, sharp) in enumerate(train_loader):\n#         blur, sharp = blur.to(device), sharp.to(device)\n\n#         fake = generator(blur)\n#         fake_detached = fake.detach()\n\n#         real_pred = discriminator(sharp)\n#         fake_pred = discriminator(fake_detached)\n\n#         d_loss_real = adv_criterion(real_pred, torch.ones_like(real_pred))\n#         d_loss_fake = adv_criterion(fake_pred, torch.zeros_like(fake_pred))\n#         d_loss = (d_loss_real + d_loss_fake) / 2\n\n#         d_optimizer.zero_grad()\n#         d_loss.backward()\n#         d_optimizer.step()\n\n#         pred_fake = discriminator(fake)\n#         g_adv = adv_criterion(pred_fake, torch.ones_like(pred_fake))\n#         g_content = content_criterion(fake, sharp)\n#         g_loss = 100*g_content + g_adv\n\n#         g_optimizer.zero_grad()\n#         g_loss.backward()\n#         g_optimizer.step()\n\n#         if i % 10 == 0:\n#             print(f\"[Epoch {epoch}] Step {i} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n\n#         del fake, fake_detached, real_pred, fake_pred, pred_fake, g_adv, g_content, d_loss_real, d_loss_fake\n#         torch.cuda.empty_cache()\n#         gc.collect()\n\n#     torch.save(generator.state_dict(), f\"checkpoints/generator_epoch{epoch}.pth\")\n\n#     evaluate_psnr_ssim(generator, val_loader, device, epoch)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T13:59:22.269808Z","iopub.execute_input":"2025-06-03T13:59:22.270021Z","iopub.status.idle":"2025-06-03T13:59:22.274890Z","shell.execute_reply.started":"2025-06-03T13:59:22.270004Z","shell.execute_reply":"2025-06-03T13:59:22.274300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_psnr_ssim(model, dataloader, device, epoch):\n    model.eval()\n    psnr_total, ssim_total = 0, 0\n    with torch.no_grad():\n        for blur, sharp in dataloader:\n            blur, sharp = blur.to(device), sharp.to(device)\n            pred = model(blur).clamp(-1, 1)\n\n            for i in range(pred.size(0)):\n                p = pred[i].cpu().numpy().transpose(1, 2, 0)\n                s = sharp[i].cpu().numpy().transpose(1, 2, 0)\n                p = (p + 1) / 2\n                s = (s + 1) / 2\n                psnr_total += peak_signal_noise_ratio(s, p, data_range=1)\n                ssim_total += structural_similarity(s, p, channel_axis=2, data_range=1)\n\n    n = len(dataloader.dataset)\n    psnr_avg = psnr_total / n\n    ssim_avg = ssim_total / n\n    print(f\"\\n[Evaluation @ Epoch {epoch}] PSNR: {psnr_avg:.2f}, SSIM: {ssim_avg:.4f}\\n\")\n\n    return psnr_avg, ssim_avg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T13:59:22.275557Z","iopub.execute_input":"2025-06-03T13:59:22.275784Z","iopub.status.idle":"2025-06-03T13:59:22.286578Z","shell.execute_reply.started":"2025-06-03T13:59:22.275758Z","shell.execute_reply":"2025-06-03T13:59:22.285998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport gc\nimport matplotlib.pyplot as plt\n\n# ... các import khác: model, loss, dataloader, optimizer, v.v.\n\nbest_ssim = 0.0  # Khởi tạo SSIM cao nhất\n\nfor epoch in range(100):\n    generator.train()\n    discriminator.train()\n\n    for i, (blur, sharp) in enumerate(train_loader):\n        blur, sharp = blur.to(device), sharp.to(device)\n        fake = generator(blur)\n        fake_detached = fake.detach()\n\n        real_pred = discriminator(sharp)\n        fake_pred = discriminator(fake_detached)\n\n        d_loss_real = adv_criterion(real_pred, torch.ones_like(real_pred))\n        d_loss_fake = adv_criterion(fake_pred, torch.zeros_like(fake_pred))\n        d_loss = (d_loss_real + d_loss_fake) / 2\n\n        d_optimizer.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n\n        pred_fake = discriminator(fake)\n        g_adv = adv_criterion(pred_fake, torch.ones_like(pred_fake))\n        g_content = content_criterion(fake, sharp)\n        g_loss = 100 * g_content + g_adv\n\n        g_optimizer.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n\n        if i % 10 == 0:\n            print(f\"[Epoch {epoch}] Step {i} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n\n        del fake, fake_detached, real_pred, fake_pred, pred_fake, g_adv, g_content, d_loss_real, d_loss_fake\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    # Lưu trọng số mô hình (state_dict)\n    torch.save(generator.state_dict(), f\"checkpoints/generator_epoch{epoch}.pth\")\n\n    # Đánh giá PSNR & SSIM trên tập validation\n    psnr_avg, ssim_avg = evaluate_psnr_ssim(generator, val_loader, device, epoch)\n\n    # Nếu SSIM tốt hơn, lưu hình ảnh và export ONNX\n    if ssim_avg > best_ssim:\n        best_ssim = ssim_avg\n\n        # Vẽ ảnh\n        with torch.no_grad():\n            blur, sharp = next(iter(val_loader))\n            blur, sharp = blur.to(device), sharp.to(device)\n            pred = generator(blur).clamp(-1, 1)\n\n            blur_img = blur[0].cpu().permute(1, 2, 0).numpy()\n            sharp_img = sharp[0].cpu().permute(1, 2, 0).numpy()\n            pred_img = pred[0].cpu().permute(1, 2, 0).numpy()\n\n            imgs = [(blur_img + 1) / 2, (pred_img + 1) / 2, (sharp_img + 1) / 2]\n            titles = [\"Blurry\", \"Predicted\", \"Sharp\"]\n            plt.figure(figsize=(12, 4))\n            for i in range(3):\n                plt.subplot(1, 3, i + 1)\n                plt.imshow(imgs[i])\n                plt.title(titles[i])\n                plt.axis(\"off\")\n            plt.tight_layout()\n            plt.show()\n\n        # ✅ Export ONNX model\n        dummy_input = torch.randn(1, 3, 256, 256).to(device)  # Thay kích thước nếu cần\n        onnx_path = f\"checkpoints/best_generator.onnx\"\n        torch.onnx.export(\n            generator,\n            dummy_input,\n            onnx_path,\n            export_params=True,\n            opset_version=11,\n            do_constant_folding=True,\n            input_names=['input'],\n            output_names=['output'],\n            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n        )\n        print(f\"✅ Exported ONNX model to {onnx_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T13:59:22.287376Z","iopub.execute_input":"2025-06-03T13:59:22.288131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef blur_detection(image: np.ndarray, size: int, threshold: float) -> bool:\n    if len(image.shape) == 3:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    dft = np.fft.fft2(image)\n    dft_shifted = np.fft.fftshift(dft)  \n\n    h, w = image.shape\n    mask = np.ones((h, w), dtype=np.uint8)\n    cx, cy = w // 2, h // 2\n    mask[cy - size:cy + size, cx - size:cx + size] = 0\n\n    dft_shifted_filtered = dft_shifted * mask\n\n    f_ishift = np.fft.ifftshift(dft_shifted_filtered)\n    img_back = np.fft.ifft2(f_ishift)\n\n    magnitude = 20 * np.log(np.abs(img_back) + 1e-5)\n    M_mean = np.mean(magnitude)\n\n    is_blurry = M_mean < threshold\n    return is_blurry, M_mean\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\n\ngenerator.eval()\nwith torch.no_grad():\n    for blur, sharp in val_loader:\n        blur, sharp = blur.to(device), sharp.to(device)\n\n        pred = generator(blur).clamp(-1, 1)\n\n        pred = postprocess_deblurred_image(pred, device)\n        # Lấy ảnh đầu tiên trong batch\n        blur_img = blur[1].cpu().numpy().transpose(1, 2, 0)\n        pred_img = pred[1].cpu().numpy().transpose(1, 2, 0)\n        sharp_img = sharp[1].cpu().numpy().transpose(1, 2, 0)\n\n        blur_img = (blur_img + 1) / 2\n        pred_img = (pred_img + 1) / 2\n        sharp_img = (sharp_img + 1) / 2\n\n        imgs = [blur_img, pred_img, sharp_img]\n        titles = ['Blurred Image', 'Predicted Image', 'Sharp Image']\n\n        plt.figure(figsize=(15, 5))\n        for i in range(3):\n            plt.subplot(1, 3, i+1)\n            plt.imshow(imgs[i])\n            plt.title(titles[i])\n            plt.axis('off')\n        plt.tight_layout()\n        plt.show()\n\n        break \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\nimport torch\nimport numpy as np\n\ngenerator.eval()\nwith torch.no_grad():\n    for blur, sharp in val_loader:\n        blur, sharp = blur.to(device), sharp.to(device)\n\n        pred = generator(blur).clamp(-1, 1)\n\n        pred = postprocess_deblurred_image(pred, device)\n        blur_img = blur[1].cpu().numpy().transpose(1, 2, 0)\n        pred_img = pred[1].cpu().numpy().transpose(1, 2, 0)\n        sharp_img = sharp[1].cpu().numpy().transpose(1, 2, 0)\n\n        blur_img = (blur_img + 1) / 2\n        pred_img = (pred_img + 1) / 2\n        sharp_img = (sharp_img + 1) / 2\n\n        crop_size = 100\n        center_x, center_y = 100, 100 \n        x_start = max(center_x - crop_size // 2, 0)\n        x_end = min(center_x + crop_size // 2, blur_img.shape[0])\n        y_start = max(center_y - crop_size // 2, 0)\n        y_end = min(center_y + crop_size // 2, blur_img.shape[1])\n\n        blur_crop = blur_img[x_start:x_end, y_start:y_end, :]\n        pred_crop = pred_img[x_start:x_end, y_start:y_end, :]\n        sharp_crop = sharp_img[x_start:x_end, y_start:y_end, :]\n\n        imgs = [blur_img, pred_img, sharp_img]\n        titles = ['Blurred Image', 'Predicted Image', 'Sharp Image']\n        plt.figure(figsize=(15, 5))\n        for i in range(3):\n            plt.subplot(1, 3, i + 1)\n            plt.imshow(imgs[i])\n            plt.title(titles[i])\n            plt.axis('off')\n        plt.tight_layout()\n        plt.show()\n\n        zoom_imgs = [blur_crop, pred_crop, sharp_crop]\n        zoom_titles = ['Zoomed Blurred', 'Zoomed Predicted', 'Zoomed Sharp']\n        plt.figure(figsize=(15, 5))\n        for i in range(3):\n            plt.subplot(1, 3, i + 1)\n            plt.imshow(zoom_imgs[i])\n            plt.title(zoom_titles[i])\n            plt.axis('off')\n        plt.tight_layout()\n        plt.show()\n\n        break  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator.eval()\nwith torch.no_grad():\n    for blur, sharp in val_loader:\n        blur, sharp = blur.to(device), sharp.to(device)\n        \n        pred = generator(blur).clamp(-1, 1)\n        pred = postprocess_deblurred_image(pred, device)\n        pred_image = pred[0].cpu().numpy().transpose(1, 2, 0)  # [H, W, C]\n        pred_image = (pred_image + 1) / 2  # Chuyển từ [-1, 1] -> [0, 1]\n        pred_image_uint8 = (pred_image * 255).clip(0, 255).astype(np.uint8)\n\n        if pred_image_uint8.shape[2] == 3:\n            pred_gray = cv2.cvtColor(pred_image_uint8, cv2.COLOR_RGB2GRAY)\n        else:\n            pred_gray = pred_image_uint8\n\n        blurry, M = blur_detection(pred_gray, size=10, threshold=5.0)\n        print(f\"Blur detection result: M = {M:.2f} → {'Blurred' if blurry else 'Sharp'}\")\n\n        import matplotlib.pyplot as plt\n        plt.imshow(pred_gray, cmap='gray')\n        plt.title(f\"Predicted Image\\nM = {M:.2f}\")\n        plt.axis(\"off\")\n        plt.show()\n\n        break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"threshold = 5.0\nsize = 10\n\nblurry, score = blur_detection(pred_gray, size=size, threshold=threshold)\nprint(f\"First predicted image: M = {score:.2f} → {'Blurred' if blurry else 'Sharp'}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test on YOLOv3","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}